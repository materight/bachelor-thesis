\chapter{Data analysis}
\label{cha:data_analysis}

This chapter illustrates the analyses conducted to get a better understanding of the nature of the problem and the available data. The first section describes the two datasets used in these analyses and the differences between them. The following sections present a series of considerations on the distributions and patterns found in the data and on the possible applications of these findings for reaching the objectives of this thesis.

\section{Datasets description}
\label{sec:datasets_description}

The datasets were generated by the RetailerIN system described in Section~\ref{sec:retailerin_system}. This system tracks, collects and analyses the movements of carts and baskets utilized by the customers. For every shopping session of a customer using a tracked asset, different information are collected and organized:
\begin{itemize}
  \item A unique \emph{session identifier}.
  \item An \emph{asset identifier} that can be used to trace the physical asset used. There are two types of available assets: \emph{carts} and \emph{baskets}.
  \item The \emph{start} and \emph{end} time of the session, i.e. when the customer enters and leaves the shop.
  \item The total \emph{dwell time} of the session (in seconds), i.e. the total time spent by the customer in the shop.
  \item An estimate of the total \emph{movement time} and the \emph{stationary time}, such that \( \text{dwell time} = \text{movement time} + \text{stationary time} \).
  \item The total \emph{distance} covered while shopping (in meters).
  \item The \emph{terminal number} in which the payment is made. There are two types of terminal: the classical \emph{assisted rolling belt terminals} and the \emph{self-checkouts}.
\end{itemize}
The datasets are exported in CSV format (Comma Separated Values). All the collected information are completely anonymous and not traceable to a specific customer.

Two distinct datasets were analyzed from two distinct supermarkets, that we would call \emph{supermarket A} and \emph{supermarket B}. Supermarket A is a large-sized North American retail store, while supermarket B is a medium-sized store in northern Italy. Each dataset covers different time periods and have some additional information that are not available for the other:
\begin{itemize}
  \item \emph{Dataset A} covers a six-month period, from the 1st of September 2019 to the 29th of February 2020, for a total of \( 235788 \) unique shopping sessions. Additional data about the products bought in each session was available, in particular:
        \begin{itemize}
          \item The \emph{total value} of the items bought (in dollars)
          \item The \emph{total number} of items bought, also called the \emph{basket size}
        \end{itemize}
  \item \emph{Dataset B} covers a two-month period, from the 6th of February to the 25th of March 2020, for a total of \( 32685 \) unique shopping sessions. Additional data about the length of every waiting queues and the average waiting times for each counter was available with a one-minute frequency. Specifically, for each interval, the measures available were:
        \begin{itemize}
          \item The \emph{average number of customers in queue}, that is the mean of the queue lengths in every instant.
          \item The \emph{total number of customers in queue}, that is the total count of customers that have joined the queue.
          \item The total number of \emph{completed sessions}, that is the total number of customers that completed the purchase and left the queue.
          \item The average \emph{time in queue} spent by the customers.
        \end{itemize}
\end{itemize}

The session dataset was binned in a 10-minute interval. This interval size was chosen because it gave a good compromise between accuracy and frequency: smaller intervals would have included too much noise and randomness to obtain significant results, while larger intervals would have not allowed to implement a fast response to sudden changes in traffic levels. In this chapter only the analyses conducted on the dataset A are shown, since the greater amount of available data gave more insightful results. The dataset B is used when the forecast accuracy of the predictive models is discussed in Chapter~\ref{cha:results}.

\section{Dwell time, inflow rate and service time distributions}
\label{sec:dwell_time_inflow_rate_service_time_distributions}

First, the measured inflow rates and dwell times were grouped by day and hour to get an overview of how the customers’ behavior changes during the week.

Since carts and baskets are used by different types of customers with different shopping behavior, they were considered separately. The heatmaps in Figure~\ref{fig:inflow_rate_heatmap} and~\ref{fig:dwell_time_heatmap} show the obtained results, confirming that carts and baskets are indeed used in very different ways. Moreover, it is already possible to identify an initial cyclic behavior: for example, the carts present an higher usage during weekends. These seasonal cycles are further analyzed in the next section.

\begin{figure}
  \begin{center}
    \inputpgf{img}{inflow_rate_heatmap.pgf}
  \end{center}
  \caption{The heatmap showing the distribution of the inflow rates, in customers/10min.}
  \label{fig:inflow_rate_heatmap}
\end{figure}

\begin{figure}
  \begin{center}
    \inputpgf{img}{dwell_time_heatmap.pgf}
  \end{center}
  \caption{The heatmap showing the distribution of the dwell times, in minutes.}
  \label{fig:dwell_time_heatmap}
\end{figure}

As explained in Section~\ref{sec:queueing_theory}, a lot of queueing theory models assume that the parameter values follow different specific probability distributions. The following distributions analyses were conducted to verify the applicability of these models.

First, the distribution of the dwell time shown in Figure~\ref{fig:dwell_time_distribution} was investigated and approximated with an \emph{Erlang distribution}. Again, carts and baskets were considered separately for the same reasons stated previously.

\begin{figure}
  \begin{center}
    \inputpgf{img}{dwell_time_distribution.pgf}
  \end{center}
  \caption{The probability density function of the dwell time, with the corresponding Erlang distribution fit (in red).}
  \label{fig:dwell_time_distribution}
\end{figure}

For the inflow rate, the distribution of the \emph{inter-entry time} was calculated, shown in Figure~\ref{fig:inter_entry_time_distribution}. The inter-entry time is the time passed between the entry of two consecutive customers. In this case, the distribution can be approximated by an \emph{exponential distribution}.

\begin{figure}
  \begin{center}
    \inputpgf{img}{inter_entry_time_distribution.pgf}
  \end{center}
  \caption{The probability density function of the inter-entry times with the corresponding exponential distribution fit (in red).}
  \label{fig:inter_entry_time_distribution}
\end{figure}

Next, the distribution of the checkouts’ service time was investigated. Since there are no available measurements of the actual service time, the \emph{basket size} \( n \), i.e. the total number of products bought in each session, was used as an approximation. Given the service time \( S \), we can define it as combination of:
\begin{itemize}
  \item a \emph{service time per item} \( S_{item} \);
  \item an \emph{extra service time} \( S_{extra} \), to take in account possible additional time spent at the checkouts, for example for complete the payment;
\end{itemize}
and we can write:

\begin{equation}
  S = n S_{item} + S_{extra}
  \label{eq:service_time}
\end{equation}

How the approximated values for \( S_{item} \) and \( S_{extra} \) are calculated is described in the next chapters. Since we considered constants \( S_{item} \) and \( S_{extra} \), the service time distribution follows the basket size distribution, shown in Figure~\ref{fig:basket_size_distribution}.

\begin{figure}
  \begin{center}
    \inputpgf{img}{basket_size_distribution.pgf}
  \end{center}
  \caption{The probability density function of the basket size with the corresponding Erlang distribution fit (in red).}
  \label{fig:basket_size_distribution}
\end{figure}

\section{Time series analysis}
\label{sec:time_series_analysis}

Different time series analysis techniques were used to extract the main components of the time series, such as trends and seasonalities. With this decomposition it was possible to verify whether additional data preprocessing were needed and to identify specific patterns and features of the time series that could be exploited by the forecasting process.

\subsection{Inflow rate autocorrelation}
\label{subsec:inflow_rate_autocorrelation}

In order to identify the number of previous values that directly influence the current value, the \emph{autocorrelation} of the inflow rate has been calculated. This measurement is useful to identify a first set of relevant features that could be used in a predictive model.

\emph{Autocorrelation} refers to the degree of correlation between the values of the same variables across different past observations in the data. It measures the linear relationship between a variable's current value and its past values. The autocorrelation values can range from -1 to +1, where +1 represents a perfect positive linear relationship (an increase in one of the values corresponds to a proportional increase in the other), -1 represents a perfect negative linear relationship, and 0 represents the absence of any linear relationship.

When data have a trend, the autocorrelations at small lags tend to be large and positive because observations nearby in time have also similar values. Therefore, the \emph{autocorrelation function} (ACF) of a trended time series tend to have positive values that slowly decrease as the lags increase. The inflow rate and dwell time trends are investigated in the next section.

When data have a seasonality, the autocorrelations will be larger for the seasonal lags (multiples of the seasonal frequency) than for the other lags~\cite{hyndman2018}. With the analysis of the inflow rate ACF in Figure~\ref{fig:inflow_rate_acf}, it was clear that the time series present two main seasonalities:
\begin{itemize}
  \item a \emph{daily seasonality}, with a cycle length of \( {\sim}144 \) lags (\( {\sim}24 \) hours), meaning that the inflow traffic peaks are repeated every day at nearly the same hours;
  \item a \emph{weekly seasonality}, with a cycle length of \( {\sim}1008 \) lags (\( {\sim}7 \) days), meaning that each day of week has a distinct traffic pattern that is repeated every week.
\end{itemize}

\begin{figure}
  \begin{center}
    \inputpgf{img}{inflow_rate_acf.pgf}
  \end{center}
  \caption{The inflow rate's ACF with the daily and weekly seasonalities.}
  \label{fig:inflow_rate_acf}
\end{figure}

The \emph{partial autocorrelation} (PACF) is the autocorrelation between two values after removing any linear dependence on the values between them. For example, the autocorrelation at lag 1 is the coefficient of correlation between \( y_t \) and \( y_{t-1} \), which is probably also the correlation between \( y_{t-1} \) and \( y_{t-2} \). But if \( y_t \) is correlated with \( y_{t-1} \), and \( y_{t-1} \) is equally correlated with \( y_{t-2} \), then we should expect to find correlation also between \( y_t \) and \( y_{t-2} \). The partial autocorrelation excludes this mutual relation from the correlation coefficient, and, at lag \( k \), it can be described as the correlation between \( y_t \) and \( y_{t+k} \) after removing the effects of \( y_{t+1}, ..., y_{t+k-1} \). The analysis of the partial autocorrelation function, shown in Figure~\ref{fig:inflow_rate_pacf}, is useful to get an insight about the number of lagged values that should be considered by a predictive regression model. Moreover, a positive autocorrelation denotes a \emph{non-stationary} time series, that is a time series whose statistical properties change over time, i.e. the trend and seasonality affect the value of the time series at different times. More precisely, if \( \{ y_t \} \) is a stationary time series, then for all \( s \), the distribution of \( ( y_t, ..., y_{t+s}) \) does not depend on \( t \)~\cite{hyndman2018}. The majority of the forecasting methods presented in Section~\ref{sec:time_series_forecasting} are based on the assumption that the time series is stationary, so the data must be \emph{differenced} before training the model to ensure stationarity. The differencing process used is explained in details in Section~\ref{subsec:artificial_neural_network_model}.

\begin{figure}
  \begin{center}
    \inputpgf{img}{inflow_rate_pacf.pgf}
  \end{center}
  \caption{The inflow rate's PACF. The positive correlation values denote a non-stationary time series.}
  \label{fig:inflow_rate_pacf}
\end{figure}

\subsection{Time series decomposition}
\label{subsec:time_series_decomposition}

Decomposing the time series into its main components is useful to find additional patterns and to validate the previous results. While in the previous section two different seasonal cycles were determined, the main goal of this section is to identify any possible trend variation among different seasons that should be taken in account when designing the final predictive model. As described in Section~\ref{sec:time_series_forecasting}, there are two types of decomposition: \emph{additive} and \emph{multiplicative}. Since the variations around the seasonal components were not proportional to the level of the time series, the additive decomposition was chosen.
First, the \emph{trend} of the inflow rate was calculated using a moving window average with a size of \( 1008 \) intervals (that is equal to one week of observations). As shown in Figure~\ref{fig:inflow_rate_trend}, the general trend seems to be stable among different months, except for September where there seems to be a greater inflow traffic. With more data available, it should be possible to identify additional monthly or yearly seasonal cycles that could be exploited to further improve the forecasting model accuracy.

\begin{figure}
  \begin{center}
    \inputpgf{img}{inflow_rate_trend.pgf}
  \end{center}
  \caption{The inflow rate's trend.}
  \label{fig:inflow_rate_trend}
\end{figure}

The same analysis was then conducted for the dwell time (see Figure~\ref{fig:dwell_time_trend}). The dwell time’s trend is not as stable as the inflow rate’s, since a slightly increase is visible during November and December.

\begin{figure}
  \begin{center}
    \inputpgf{img}{dwell_time_trend.pgf}
  \end{center}
  \caption{The dwell time's trend.}
  \label{fig:dwell_time_trend}
\end{figure}

\subsection{Service time approximations}
\label{subsec:service_time_approximations}

As described in Section~\ref{sec:dwell_time_inflow_rate_service_time_distributions}, an approximation of the checkouts' service time \( S \) can be obtained using the basket size \( n \) with Eq.~(\ref{eq:service_time}): \( S = n S_{item} + S_{extra} \). However, this approximation is feasible only if the number of products bought in each session is available. Unfortunately, that is not the case for every store, so other approximations were investigated.

An alternative approach is to use the average number of customers served by each terminal. Since the service rate should represent the maximum service capacity of each terminal, the periods where the counters spent some time in \emph{idle}, i.e. without serving any customer, had to be excluded from this approximation. Therefore, only the intervals with a measured queue length greater than a predetermined threshold were considered. This threshold was determined by analyzing the measured \emph{outflow rate} and comparing it to the \emph{total service rate} (with \(\text{total service rate} = \text{service rate} \cdot \text{number open terminals} \)). When the two measurements have similar values, the maximum service capacity has been reached and the checkouts have probably spent no time in idle, thus the number of customer served in that interval should give an accurate approximation of the actual service rate. However, again, this approach is implementable only if the dataset contains the queues’ status.

In order to get an approximation that could be used in every setting, with data that is available independently from the store, the relation between the service and dwell times was researched. As shown in the Section~\ref{sec:dwell_time_inflow_rate_service_time_distributions}, both measurements follow an erlang distribution. Intuitively, a longer dwell time should correspond to a greater amount of products bought (i.e. a greater basket size) and therefore a longer service time. However, this hypothesis was confuted by Berman et al.~\cite{berman}, which showed that there is no such relation between these two measurements, and by the correlation coefficient that resulted in a value of approximately \( 0.24 \). This is probably because the majority of the dwell time is spent looking and deciding and there could be other unrelated factors that influence the service time, such as the use of coupons and credit cards during the payment.

The last approach presented was given by the analysis of the time spent in idle by each terminal \( t_i \). To do so, the \emph{inter-exit times} \( \Delta t_e \) were calculated, that are the times passed between the consecutive exits of two customers served by the same counter:
\begin{itemize}
  \item if \( \Delta t_e > S \), the checkout has probably spent some time in idle, in particular \( t_i = \Delta t_e - S \). If \( \Delta t_e \gg S \) the checkout is probably closed for that interval;
  \item otherwise, if \( \Delta t_e \le S \), by the time the session’s purchase has been completed by the checkout, another customer is ready to be processed, and therefore \( t_i = 0 \). In this case there is probably a waiting queue for that counter.
\end{itemize}

The hours with peak outflow traffic, i.e. with minimum inter-exit rates, were taken in consideration to approximate the value of \( S \). This is because when the inter-exit times are low, there is probably a queue and consequently the checkouts should have spent no time in idle, meaning that \( t_i = 0 \) and \( S = \Delta t_e \). If the data about the products bought in each session is available, the approximated service time \( S \) thus obtained could be used to determine the values of \( S_{item} \) and \( S_{extra} \) introduced before.

\subsection{Service rate and open terminals count approximation}
\label{subsec:service_rate_and_open_terminals_count_approximation}

Once an approximation of the service time \( S \) has been determined, the \emph{service rate} \( \mu \), that is the number of customers served in a time interval \( \Delta t \), can be obtained as:
\begin{equation}
  \mu = \frac{\Delta t}{S}
  \label{eq:service_rate}
\end{equation}

Intuitively, the total service rate should be directly influenced by the number of open terminals, considering that with more terminals the total service capacity increases. For this reason, the relation between these two measurements were investigated. Since a direct measurement of the total service rate was not available and it is equal to the total number of customers that leave the checkout area and then, after a small delay, exit the store, the \emph{outflow rate} was used as approximation. The correlation between the two measures resulted in a value of approximately \( 0.92 \), meaning that there is a strong linear relationship between them. To further verify this relation, the mean of the service rates per terminal \( \bar{\mu} \) was used to obtain the outflow rate \( o(t) \), as:
\begin{equation}
  o(t) = c(t) \cdot \bar{\mu} = c(t) \cdot \frac{\Delta t}{\bar{S}}
\end{equation}

A comparison between the approximated and real outflow rate can be seen in Figure~\ref{fig:outflow_approximation}. The accuracy of the results shows that a time-independent service time can be considered, since a constant value for \( S \) gives a good approximation of the actual outflow.

\begin{figure}
  \begin{center}
    \inputpgf{img}{outflow_approximation.pgf}
  \end{center}
  \caption{Example of the measured outflow rate with the corresponding service-rate-based approximation.}
  \label{fig:outflow_approximation}
\end{figure}

\medskip
\clearpage