\chapter{Data analysis}
\label{cha:data_analysis}

This chapter describes the analysis conducted to get a better understanding of the available data and the nature of the problem. The first section describes the data that has been used in this thesis

\section{Datasets description}
\label{sec:datesets_description}
The datasets were generated by the RetailerIN platform developed by Thinkinside. The RetailerIN system tracks, collects and analyses movements of carts and baskets utilized by customers. For every shopping session of the customers using a tracked asset, different informations are collected and organized:
\begin{itemize}
  \item A unique session identifier
  \item An asset identifier. There are two types of available assets: carts and baskets
  \item The start and end time of the session, i.e. when the customer enters and leaves the shop
  \item The dwell time of the session, i.e. the total time spent by the customer in the shop
  \item An estimate of the movement time and the stationary time
  \item The total distance covered while shopping
  \item The terminal in which the payment happened. There are two types of terminal: classical assisted rolling belt terminals and self-checkouts.
\end{itemize}

The datasets are exported in CSV format (Comma Separated Values). All the collected informations are completely anonymous and not traceable to a specific customer.
Two dataset (A and B) were analyzed from two different supermarkets, that we would call supermarket A and supermarket B. Supermarket A is a large north american retail store and supermarket B is a medium-sized store in northern Italy. Each dataset covers different periods, and there are some additional information not available for both supermarkets:
\begin{itemize}
  \item The dataset A goes from 1st September 2019 to 29th February 2020, for a total of 235788 unique sessions. Additional data about the products bought in each session was available, specifically:
        \begin{itemize}
          \item The total value of the items bought
          \item The total number of items bought, also called the basket size
        \end{itemize}

  \item The dataset B goes from  6th February to 25th March 2020, for a total of 32685 unique sessions. Additional data about average queues length and waiting times of each checkout was available with a one-minute frequency, specifically:
        \begin{itemize}
          \item The average number of customers in queue, that is the
          \item The total number of customers in queue
          \item The total number of completed session
          \item The average time in queue spent by each customer
        \end{itemize}
\end{itemize}
The session dataset was binned in a 10-minutes interval. In this chapter only the analysis results for the dataset A are shown, since the greater amount of available data allowed for more insightful results.

\section{Inflow rate, dwell time and service time distributions}
\label{sec:rates_distributions}
First, the measured inflow rates and dwell times were grouped by day and hour and plotted in heatmaps to get an overview of the different customer’s behavior during the week. Carts and baskets were considered separately.

Then, the following distributions were analysed and fitted with known distributions to find out if the queueing theory techniques could be applied to this specific retail setting.
First, the distribution of the dwell time was investigated. The different assets types were considered separately since they present very different behaviours. An approximation of the distribution was given by the Erlang distribution (in red).

For the inflow rate, the distribution of the inter-arrival times was calculated. The inter-arrival time is the time passed between the arrival in the shop of two consecutive customers. The distributions can be approximated with an exponential distribution (in red).

Since there are no available measurements about the real service time of the checkouts, the basket size (number of products bought in each session) $$ n $$ was used as an approximation. Given the service time $$ S $$, we can define a fixed service time per item $$ S_{item} $$ and a fixed extra service time $$ S_{extra} $$ (to take in account additional time spent at the checkouts, e.g. for payment) such that:

$$ S = n S_{item} + S_{extra} $$
How the approximated values for $$ S_{item} $$ and $$ S_{extra} $$ were calculated is described in the next chapters.
The obtained distribution with the corresponding Erlang distribution fit is presented below.


\section{Time series analysis}
\label{sec:time_series_analysis}
Different time series analysis techniques were used to extract the main components of the time series.

\subsection{Inflow rate autocorrelation}
\label{subsec:autocorrelation}
To identify the number of previous values that directly influence the current value, the autocorrelation of the inflow rate has been calculated. This measurement is useful to identify a first set of relevant features to be used in the forecast model.
Autocorrelation refers to the degree of correlation between the values of the same variables across different past observations in the data. It measures the linear relationship between a variable's current value and its past values. The autocorrelation values can range from -1 to +1, where +1 represents a perfect positive linear relationship (an increase in one of the values leads to a proportional increase in the other value), -1 represents a perfect negative linear relationship and 0 represents the absence of any linear relationship. When data have a trend, the autocorrelations for small lags tend to be large and positive because observations nearby in time are also nearby in size. So the ACF of trended time series tend to have positive values that slowly decrease as the lags increase. When data have a seasonality, the autocorrelations will be larger for the seasonal lags (at multiples of the seasonal frequency) than for other lags [3]. With the analysis of the inflow rate autocorrelation function (ACF), it was clear that the time series present two main seasonalities:
\begin{itemize}
  \item a \emph{daily} seasonality, with a cycle length of \( \sim150 \) lags ( \( \sim24 \) hours), meaning that the periods with more or less traffic are nearly the same for each day.
  \item a \emph{weekly} seasonality, with a cycle length of \( \sim1000 \) lags (\( \sim7 \) days), meaning that there is a strong relation between the inflow rates for the same days in different weeks.
\end{itemize}

The partial autocorrelation (PACF) is the autocorrelation between two values after removing any linear dependence on the values between them. That is, the partial autocorrelation at lag $$ k $$ is the correlation between $$ y_t $$ and $$ y_{t+k} $$ after removing the effects of lags $$ y_{t+1}, ..., y_{t+k-1} $$. The analysis of the partial autocorrelation is useful to get an insight about the number of lagged values that should be considered by the regression model. Moreover, a positive autocorrelation denotes a non-stationary time series, that is a time series whose statistical properties change over time, i.e. the trend and seasonality affect the value of the time series at different times. More precisely, if $$ \{ y_t \} $$ is a stationary time series, then for all $$ s $$, the distribution of $$ ( y_t, ..., y_{t+s}) $$ does not depend on $$ t $$ [3]. The majority of forecasting methods presented in the previous chapter are based on the assumption that the time series is stationary, so the data must be differenced before training the model. How the time series was made stationary is explained in Chapter 4.1.3.

\subsection{Time series decomposition}
\label{subsec:time_series_decomposition}
Decomposing the time series into different components is useful to find additional patterns and to validate the previous observations. In the previous section two different seasonal cycles were determined, the main goal of this section is to identify possible trend variations among different seasons that should be taken in account when designing the final predictive model. As described in Chapter 2.2 there are two type of decomposition: additive and multiplicative. The additive decomposition was chosen since the variation around the seasonal components were not proportional to the level of the time series.
First, the trend of the inflow rate was calculated using a moving window average of length $$ 1008 $$ intervals (that is equal to one week). As shown in the graph, the general trend seems to be stable among different months, except for September where there seems to be a greater inflow traffic. With more data available, it should be possible to identify additional monthly or yearly seasonal cycles that could be exploited to further improve the prediction model accuracy.

The same analysis was then completed with the dwell time. The dwell time’s trend is not as stable as the inflow rate’s, since a slightly increase is visible during December.

\subsection{Service time approximations}
\label{subsec:service_time_approximations}
As described in Chapter 3.2, an approximation of the service time $$ S $$ at the checkouts can be obtained using the basket size by:

$$ S = n S_{item} + S_{extra} $$

Unfortunately, this approximation is feasible only if data about the products bought in each session is available, that is not the case for every store, so other approximations were investigated.
An alternative approach is to use an average of the count of served customers by each terminal, but again, this information is available only if the dataset contains the queues’ status. Since the service rate should represent the maximum service capacity of each terminal, only the intervals with a measured queue length greater than a predetermined threshold were considered, to exclude from the approximation the idle time. This threshold was determined by analysing the measured outflow rate and comparing it to the total service rate (= service rate * open terminals). When the two measurements are similar, the maximum service capacity has been reached and the checkouts should spend no time in idle, so the number of customer served in that interval should give an accurate approximation.
To get an approximation that could be used in every settings, i.e. with data available in every store, the relation of the service rate with the dwell times was investigated. As shown in the previous chapters, both the dwell time and the service time follow an erlang distribution. Intuitively, a longer dwell time should correspond to more items bought and thus a longer service time, and it could be used to get an approximation of the expected service time. To understand this relation, the correlation between the two measurements was calculated, with a value of only $$ 0.24 $$. Moreover, Berman et al. [1] shown that there is no such relation between the two informations. This means that the dwell time has no influence over the service time, probably because the majority of the dwell time is spent looking and deciding. Also there could be other unrelated factors that influence the service time, such as use of coupons and credit cards.
Another approach was given by the analysis of the time spent in idle $$ t_i $$ by each terminal, that is the time in which a terminal does not serve any customer. To do so, the inter-exit times $$ \Delta t_e $$ were calculated, that is the time passed between two consecutive leaving customers served by the same terminal:
\begin{itemize}
  \item If $$ \Delta t_e > S $$, the checkout has probably spent some time in idle, in particular $$ t_i = \Delta t_e - S $$. If $$ \Delta t_e \gg S $$ the checkout is probably closed for that interval.
  \item If $$ \Delta t_e \le S $$, by the time the session has been processed by the checkout, another customer is ready to be processed, so there is probably a queue.
\end{itemize}

The hours with peak outflow traffic, i.e. with minimum inter-exit rates, were taken in consideration to approximate the value of $$ S $$. That is because when the inter-exit times are low, there is probably a queue and consequently the checkouts should have spent no time in idle, meaning that $$ t_i = 0 $$ and $$ S = \Delta t_e $$. If the data about the products bought in each session is available, the approximated service time $$ S $$ could be used to determine the values of $$ S_{item} $$ and $$ S_{extra} $$.

\subsection{Service rate and open terminals count approximation}
\label{subsec:service_rate_and_open_terminals_count_approximation}
Once an approximation of the service time $$ S $$ has been determined, the service rate $$ \mu $$, that is the number of customers served in a time interval $$ \Delta t $$, can be obtained as:

$$ \mu = \frac{\Delta t}{S}$$

Intuitively, the total service rate should be directly influenced by the number of available terminals, considering that with more terminals open the total service capacity of the checkouts increases. For this reason, the relation between these two measurements were investigated. Since a direct measure of the total service rate was not available, the outflow rate was used as an approximation. This because the total service rate equals the total number of customers that leave the checkout area and that exit the store after a small delay. The correlation between the two values was then calculated, with a value of $$ 0.92 $$, meaning that there is a strong linear relationship between these two measurements.
To verify this relation, the mean of the service rates per terminal $$ \bar{\mu} $$ was used to obtain the outflow rate $$ o(t) $$ as:
$$ o(t) = c(t) \cdot \bar{\mu} = c(t) \cdot \frac{\Delta t}{\bar{S}} $$
A comparison between the approximated and real outflow rate is shown below. The accuracy of the results means that a time-independent service time can be considered, since a constant $$ S $$ value gives a good approximation of the real outflow.


\clearpage