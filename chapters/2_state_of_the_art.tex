\chapter{State of the art}
\label{cha:state_of_the_art}

This chapter discusses the available related literature. First, various solutions to similar problems from other researches are described. All these researches falls into the broader discipline of \emph{Operational Research} (OR), and mainly into two specific sub-fields: \emph{Time Series Forecasting} and \emph{Queueing Theory}. Since this thesis is also focused on these types of approach to the problem, the state-of-the-art techniques for those fields are discussed in the following two sections.

Some of the terminology that will be used in this chapter:
\begin{itemize}
  \item \emph{Inflow/outflow rate}: number of customers that enter/exit the supermarket in a given time interval.
  \item \emph{Arrival rate}: number of customers that arrive at the checkouts in a given time interval.
  \item \emph{Service rate}: maximum number of customers that can be served by each checkout in a given time interval.
  \item \emph{Dwell time}: time spent by a customer inside the store.
  \item \emph{Basket size}: total number of items bought in a single shopping session.
\end{itemize}

\section{Literature review}
\label{sec:literature_review}

There are various studies that try to analyze and predict the queue length in different settings, e.g. airports or hospitals, but little was found in the specific context of supermarkets or regarding staff optimization. While these unrelated researches can still give useful insight, it is difficult to adapt the proposed solutions to the context of this thesis, since every distinct setting introduces different constraints. Therefore, this section focuses on the findings of the supermarket-focused studies.

In 2004, Berman et al.~\cite{berman} developed a system that manages the switching of workers between a "front room", where the checkouts are located, and a "back room", that is the rest of the shop. This system processes real-time data about the count of customers either in front and back rooms, with the goal of minimizing the customer's waiting time while maintaining an adequate level of staffing in the back room for store operations. To accomplish this performance objective, they first defined a maximum \emph{waiting time threshold} that shall not be exceeded, and a minimum \emph{time-average worker complement threshold} to complete all the back room work. They used a \( M/M/\infty \) queueing model to determine the performance of the system with a given number of front room workers. With this method, they were able to suggest in real-time the optimal strategy of switching workers from one room to another, minimizing the average waiting time while satisfying the threshold constraints.

Aksu H.~\cite{aksu} main goal was to optimize the idle time of the staff operating at the checkouts while maintaining the waiting queue length under a predetermined threshold. In 2008, he proposed a model based on the inflow of customers at the entrance and at the checkout area, the current waiting queue length and the number of available checkouts. He divided the supermarket area into a shop and a checkout area, and used a video-based system to count the customers moving between them. He then developed a prediction model to calculate a realistic forecast of customer's dwell times. The inward flow into the checkout area was predicted using the inward flow into the shop, since the inward flow into the shop appears with a delay at the checkout area, and this delay is the dwell time. To consider non-standard events and increase the accuracy, the predictions were adjusted with real-time measurements. Queueing theory was then applied to predict the average waiting time at the checkouts. With this information, the system was able to suggest to either open or close counters to meet a predefined minimum and maximum acceptable queue length. To avoid closing and opening too frequently, the system was also able to decide in how many cases the defined waiting queue length could be exceeded, e.g. it was acceptable to exceed the target value for a short period of time. By following the model’s suggestion, the supermarket used for testing decreased the staff’s time spent in idle by 62.17\% on average, while increasing the queue length by only 0.2 (from 1.9 to 2.1 customers in queue on average).

\section{Time series forecasting}
\label{sec:time_series_forecasting}

A \emph{time series} is a collection of observations made sequentially through time. Since the customers traffic levels can be seen as points in time, many time series analysis techniques and prediction models can be used. \emph{Time series forecasting} is the use of a model to predict future values based on previous observations. A time series can be decomposed into three main components:
\begin{itemize}
  \item \emph{Trend} (\( T_t \)): linear/nonlinear, increasing/decreasing behavior of the series over time.
  \item \emph{Seasonality} (\( S_t \)): repeating patterns or cycle behavior over time.
  \item \emph{Residual} (\( R_t \)): variability of the observations not explainable by the model.
\end{itemize}

There are two types of decomposition: \emph{additive}, where the time series would be written as \( y_t = T_t + S_t + R_t \), and \emph{multiplicative}, where \( y_t = T_t \cdot S_t \cdot R_t \). The additive decomposition is the most appropriate if the magnitude of the seasonal fluctuations, or the variation around the trend-cycle, does not vary with the level of the time series. When the variation in the seasonal pattern appears to be proportional to the level of the time series, then a multiplicative decomposition is more appropriate.

Once the main components of the time series have been determined, a predictive model can be defined. There are many different time series forecasting methods available, the most popular and effective are presented in the next sections.

\subsection{Exponential smoothing}
\label{subsec:exponential_smoothing}

In the \emph{Simple Exponential Smoothing} (SES) method, the forecasted values are based on a weighted average of the previous values, where the most recent observations are given more importance using larger weights~\cite{brown}.

A more complex implementation of the same principle is the \emph{Holt-Winters Exponential Smoothing} method, which is able to decompose the time series into a level, trend and seasonal component, giving a more precise forecast~\cite{holt}. However, it is not possible to model more complex series with multiple seasonality patterns, as in a supermarket inflow rate which presents daily and weekly seasonalities.

Taylor’s \emph{Double Seasonal Exponential Smoothing} method was developed to forecast time series with two seasonal cycles: a short one that repeats itself many times within a longer one~\cite{taylor}.

\subsection{Artificial Neural Networks}
\label{subsec:artificial_neural_networks}

\emph{Artificial Neural Networks} (ANN) allow to model complex nonlinear relationships between the target variable and its predictors. With time series data, subsequent lagged values can be used as inputs, in what is called an \emph{Autoregressive Neural Network} (AR-NN)~\cite{tang}. With seasonal data, it is also useful to include in the input the last observed values from the past seasons~\cite{hyndman2018}. In general, to compute a forecast value \( \hat{y}_{t+1} \) for time \( t+1 \), the corresponding input sample can be written as:
\[
  (y_{t}, y_{t-1}, ..., y_{t-p}, y_{t-m}, y_{t-2m}, ..., y_{t-Pm})
\]
The notation \( \text{AR-NN}(p, P, k)_m \) is used to indicate an autoregressive neural network, with:
\begin{itemize}
  \item \( p \): number of lagged observations in input;
  \item \( P \): number of seasonal lagged observations in input;
  \item \( k \): number of neurons in the hidden layer;
  \item \( m \): number of time steps in a seasonal period.
\end{itemize}

For forecasting two steps ahead, the result of one-step forecast can be used in input. Therefore, once \( \hat{y}_{t+1} \) has been determined, \( \hat{y}_{t+2} \) can be computed by giving in input the sample:
\[ (\hat{y}_{t+1}, y_{t}, y_{t-1}, ..., y_{t-p-1}, y_{t-m}, y_{t-2m}, ..., y_{t-Pm}) \]

This method can be then applied recursively to obtain a three-step, four-step, etc... forecast.

\subsection{ARIMA models}
\label{subsec:arima_models}
The \emph{AutoRegressive Integrated Moving Average} (ARIMA) models provide another technique for time series forecasting. Exponential smoothing and ARIMA models are the two most widely-used time series forecasting methods, and provide complementary approaches to the problem. While exponential smoothing models are based on a description of the trend and seasonality in the data, ARIMA models aim to describe the autocorrelations in the data~\cite{jenkins}:
\begin{itemize}
  \item \emph{AutoRegressive} means that the model uses the relationship between an observation and some number of lagged previous values to generate a linear regression model;
  \item \emph{Integrated} refers to the use of differencing of raw observations (e.g. subtracting an observation from the previous time step value) in order to make the time series \emph{stationary}. A stationary time series has constant and time-independent mean, variance and autocorrelation;
  \item \emph{Moving Average} means that the model uses the dependency between an observation and the residual error from a moving average model applied to lagged observations.
\end{itemize}
Accordingly, the parameters of the ARIMA model are defined as follows:
\begin{itemize}
  \item \( p \): autoregressive order, the number of lagged observations included in the model;
  \item \( d \): degree of differencing, the number of times that the raw observations are differenced;
  \item \( q \): moving average order, the size of the moving average window.
\end{itemize}
This simple version of ARIMA can only be used to forecast non-seasonal time series, it can however be extended to model seasonal patterns with the Seasonal ARIMA (SARIMA)~\cite{hyndman2018}. There are four additional seasonal parameters not part of ARIMA that must be configured:
\begin{itemize}
  \item \( P \): seasonal autoregressive order;
  \item \( D \): seasonal degree of differencing;
  \item \( Q \): seasonal moving average order;
  \item \( m \): number of time steps in a seasonal period.
\end{itemize}
A \( \text{SARIMA}(p,0,0)(P,0,0)_m \) model is equivalent to an \( \text{AR-NN}(p,P,0)_m \) model, but with additional parameters restrictions to ensure stationarity.

However, only a single seasonal effect can be modelled with SARIMA. To model more seasonalities, a SARIMAX model must be used, since it supports the use of additional \emph{exogenous variables}: the multiple seasonal effects can be captured by Fourier terms and used as external variables. By doing this the overall forecast accuracy is increased and additional exogenous variables can be added to the model to take in account other external factors (e.g. weather or holidays).

\subsection{TBATS models}
\label{subsec:tbats_models}

An alternative approach proposed in 2011 by De Livera et al.~\cite{de_livera} uses a combination of Fourier terms with an exponential smoothing model and a \emph{Box-Cox transformation}~\cite{box}, in a completely automated manner. The main advantage of a TBATS model over a SARIMAX model with Fourier terms is that the seasonality is allowed to change slowly over time, while the harmonic regression terms force the seasonal patterns to repeat periodically without changing. However, one drawback of TBATS models is that they are very slow to estimate, especially when the number of available observations is large, since they will consider various alternatives and fit different models in order to find the best one.

\section{Queueing theory}
\label{sec:queueing_theory}

\emph{Queueing theory} is the mathematical study of waiting lines, or queues. A queueing model is constructed so that queue lengths and waiting times can be predicted by the traffic and service levels. There are various types of queueing models, with different constraints and properties. The \emph{Kendall's notation}, proposed in 1953 by Kendall~\cite{kendall}, is the standard system used to describe and classify these models using four factors, written as \( A/S/c/K \), where:
\begin{itemize}
  \item \( A \) denotes the time distribution between arrivals to the queue;
  \item \( S \) denotes the service time distribution;
  \item \( c \) denotes the number of available servers;
  \item \( K \) denotes the maximum capacity of the queue. If not specified it is assumed \( K = \infty \).
\end{itemize}

The best representation of a supermarket queue is given by the \( M/M/c \) model, where \( M \) denotes a \emph{Markovian process}, meaning that the inter-arrival times and the service times of the \( c \) servers are exponentially distributed. The customers are served in FCFS order (First Come First Served).

The \emph{utilization factor} \( \rho = \lambda/c\mu \) describes the proportion of total service capacity being used in the system, where:
\begin{itemize}
  \item \( \lambda \) is the average arrival rate;
  \item \( \mu \) is the average service rate of a single server;
  \item \( c \) is the number of available servers.
\end{itemize}
If \( \rho \ge 1 \), i.e. the arrival rate exceeds the total service capacity, the queue will grow indefinitely, but if \( \rho < 1 \), the system is considered stable and the steady-state average queue length can be calculated.

The main issue with this stationary approach in the context of supermarkets is that the arrival rate and the number of available terminals cannot be expressed by probability distributions with constant mean, since they are strongly time-dependent and therefore non-stationary. We can denote such time-varying model as \( M(t)/M/c(t) \), where the service rate is instead considered to be time-independent. Moreover, in a real situation it is possible for the utilization factor to exceed \( 1 \) for a short period of time (called \emph{overloading}), in which the queue length increases, and after that, when \( \rho < 1 \), the system returns slowly to its equilibrium.

A common approach for dealing with time-varying rates is the \emph{Stationary Independent Period by Period} (SIPP) approach, where the analysis is conducted on small intervals considered independently. For each interval, a different \( M/M/c \) model with constant arrival rates and constant number of available servers is created and solved with the stationary approach. However, in 2001 Green et al.~\cite{green2001} showed that the commonly used SIPP approach is inaccurate for parameter values corresponding to many real situations, even when the time-dependent variations are small and especially for systems which operate near the critical load. Moreover, this stationary analysis requires the arrival rate to be strictly smaller than the service rate, i.e. \( \rho < 1 \) must hold for every interval, while many real systems can be temporarily overloaded.

Green et al.~\cite{green1991} proposed an easy-to-compute approximation for determining long run average performance measures for multi-server queues with periodic arrival rates: the \emph{Pointwise Stationary Approximation} (PSA). This approximation was obtained by integrating over time, that is taking the formula for the stationary performance measure with the arrival rate that applies at each point in time. They empirically showed that this approximation is a tight upper bound for the true number of customers in the queue and that it is very accurate under certain parameters values. However, again, the PSA approach can only be applied to systems where \( \rho < 1 \) hold, i.e. temporal overloading in some intervals is strictly forbidden~\cite{stolletz}.

In 2008, Stolletz~\cite{stolletz} proposed an improvement of the SIPP approach, the \emph{Stationary Backlog-Carryover} (SBC) approach. This method was designed for systems with temporal overloading, thus the models constructed for every consecutive period are no longer independent from each other. Contrary to the SIPP approach that independently applies a \( M/M/c/\infty \) model to each interval, SBC utilizes a \( M/M/c/c \) model, also known as the \emph{Erlang’s B model}, in which a maximum number of \( c \) customers can be in the queue at any time, and any further arrival to the queue is considered \emph{blocked} (i.e. lost). These blocked customers are then carried over into future periods. This is obtained by defining an \emph{artificial arrival rate} \( \widetilde{\lambda}(t) \) that consists of both the original arrival rate \( \lambda(t) \) and a \emph{backlog rate} \( b(t-1) \) of the previous period, that is the rate of blocked customers leaving the \( M/M/c/c \) system at time \( t-1 \). This artificial arrival rate can be then used with a modified \( M/M/c/\infty \) model to determine the average queue length.

Once the average queue length has been determined with one of the aforementioned methods, the average waiting time in the queue can be calculated using \emph{Little’s Law}, proposed by Little in 1961~\cite{little}. This law states that the average number of customers in a stationary system \( Ls \) is equal to the arrival rate \( \lambda \) multiplied by the average time spent by the customers in the system \( Ws \):

\begin{equation}
  Ls = \lambda \cdot Ws
  \label{eq:little_law}
\end{equation}

This equation is applicable only with stationary systems, but it can be extended to non-stationary systems with the SBC approach by using the artificial arrival rate \( \widetilde{\lambda} \) instead of \( \lambda \).

\clearpage